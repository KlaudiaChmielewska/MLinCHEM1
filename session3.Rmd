---
title: "__ML in chem__"
subtitle: "*Lesson 3*"
author: "__*Klaudia Chmielewska*__"
output:    
  html_document:
    theme: darkly
    df_print: paged
    highlight: tango #code highlighter
    toc: true # generates a table of contents (based on # and ## lines)
    number_sections: false # table of contents numbers
    toc_float: yes # floating table of contents
    css: style.css 
---

```{r setup, include=FALSE, eval=FALSE}

knitr::opts_chunk$set(echo = TRUE)
```

\

# *<span style="text-decoration:underline">Python vs R</span>*
\

## Which one is better?
\

People tend to be 'practical' when it comes to saving time. Whether it's ordering online so that you don't have to go anywhere, or reading a book summary instead of diving into the book - we feel satisfied, efficient and proud, further spending saved time on our hobbies or other things that bring us joy.
\

Same goes for learning. Even those psyched over a subject can still feel the urge to take a shortcut. In Poland we sometimes say that laziness is the driving force of mankind. Perhaps if humans were not lazy, we wouldn't feel a push towards change.
\

Having that in mind, you probably figured out that the answer to the question of this section will be - *it depends*.
In the same way that a 3in1 shampoo won't be the best choice for our hair in the long run, *Python cannot be a all-in-one programming language*.

\

We'll go over the possibilities of each language and discuss useful packages.

Interestingly, you can also do machine learning in other languages, like C/C++ or Java. Actually, languages like Lua, Lisp and Haskell offer quite a similar set of possibilities as Python and R, however never went into fashion.
Other possibility is Julia, a Jupyter notebook option, which works with the speed of C, on the basis of Python/R scripts.
\

In the end, it all goes down to preference, although many indicate that R was created with having statistics in mind, hence seems to be more versatile and fit for the job. However, Python packages for statistics and machine learning are being developed each year.
\
\

## Why R?

\

- preferred by most of the academic community
- designed for statistical analysis
- considered best for statistics and data visualization
- over 7 thousand packages for data analysis and statistical modeling research (which lets less users with less programming experience perform powerful calculations)
- *rmarkdown* and *Shiny* for creating reports (easily and quickly obtainable reports, R supports LaTeX, hence offers a possibility of creating interactive graphs)
- given that Python libraries are usually inspired by R packages, it is more comfortable using R sometimes
- interestingly, most econometricians only work with R as it exclusively has the package necessary
- if you encounter a problem along the way, it's easier to find a solution for R code rather than Python, as more data scientists preffer R in general (it is also connected to the fact that R offers more possibilities in specific cases and has an established group of users, able to answer questions on public forum etc.)
- computationally slower language compared to Python, especially if the code is written poorly


\
\

## Why Python?

\

- more popular in the industry
- some state it is better with larger data sets (while others say otherwise...)
- more versatile than R (besides statistics and reports making, it's also good for web and app development, database connectivity; some also claim it is better for machine learning)
- results are not as detailed and eye-catching as the ones from R
- it seems learning and using Python ends up in finding more job opportunities
- a little harder to learn than R

\
\


***

\
\




## Useful Python libraries

\
\

### Data wrangling 
\

* NumPy
* SciPy
* Pandas

\

### Visualization
\

* Matplotlib [quite low-lvl - requires you to write more code than R]
* Seaborn [for heat maps, dependent on Matplotlib]
* Bokeh [interactive graphs, independent of matplotlib]
* Plotly [web-based graphical toolkit]

\

### Machine Learning
\

* SciKit-Learn
* Keras
* TensorFlow
* Theano




### Other

* Scrapy
* NLTK
* Gensim
* Statsmodels







# *<span style="text-decoration:underline">MLR - Coding</span>*
\
\

## Python

\


``` {r python, eval=FALSE}
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('your_data_set.csv')

# Access specific rows and columns (change the numbers)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 4].values

# Encoding the Independent Variable
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[:, 3] = labelencoder_X.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder()
a = onehotencoder.fit_transform(X[:,[3]]).toarray()


# Avoid the dummy variable Trap
a=a[:,1:]

# Add encoded data into X
X = X[:,:3]
X=np.concatenate((X, a),axis=1)

# Split the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
"""from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train)"""

# Linear Model
from sklearn.linear_model import LinearRegression
regressor=LinearRegression()
regressor.fit(X_train,y_train)

y_pred=regressor.predict(X_test)

# add column of 1's in X
import statsmodels.formula.api as sm
X=np.append(arr=np.ones((50,1)).astype(int),values=X,axis=1)

import statsmodels.api as sm1

#backward Elimination
X_opt=np.array(X[:,[0,1,2,3]],dtype = float)
regressor_OLS=sm1.OLS(endog=y,exog=X_opt).fit()
regressor_OLS.summary()

#forward selection
X_opt2=np.array(X[:,[0]],dtype = float)
regressor_OLS2=sm1.OLS(endog=y,exog=X_opt2).fit()
regressor_OLS2.summary()


#Visualizing the test set result  
from matplotlib.colors import ListedColormap  
x_set, y_set = x_test, y_test  
x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),  
nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))  
plm.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),  
alpha = 0.75, cmap = ListedColormap(('red','green' )))  
plm.xlim(x1.min(), x1.max())  
plm.ylim(x2.min(), x2.max())  
for i, j in enumerate(nm.unique(y_set)):  
    plm.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],  
        c = ListedColormap(('red', 'green'))(i), label = j)  
plm.title('MLR algorithm(Test set)')  
plm.xlabel('Label1')  
plm.ylabel('Label2')  
plm.legend()  
plm.show()

```

\
\


## R

### Multivariate multiple regression

\
\

An extension to MLR, of which an interesting description is provided [here](https://scse.d.umn.edu/sites/scse.d.umn.edu/files/cassiequickfinalpaper.pdf). 
\


Multivariate regression: several dependent variables with different variances
Multiple regression: only one dependent variable y

\
\

Let's try to analyze a particular data set, from a published paper regarding wine:
\


P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. 
Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.
\
\


#### Data Understanding

Wine variants of Portuguese "Vinho Verde" are analysed with regards to their chemical properties. Finally, we are interested how these chemical properties influence wine quality.


These are our independent variables:

1 - fixed acidity 
2 - volatile acidity 
3 - citric acid 
4 - residual sugar 
5 - chlorides 
6 - free sulfur dioxide 
7 - total sulfur dioxide 
8 - density 
9 - pH 
10 - sulphates 
11 - alcohol 

This is our dependent variable:

12 - quality (score between 0 and 10)
\

\

``` {r Rcode, eval=FALSE}


## Packages

We load required packages.

```{r}
library(dplyr)
library(tibble)
library(tidyr)
library(ggplot2)
library(corrplot)
library(car)
library(caret)
```
\
\

#### Data Import

```{r}
# if file does not exist, download it first
file_path <- "./data/winequality-red.csv"
if (!file.exists(file_path)) {
  dir.create("./data")
  url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
  download.file(url = url, 
                destfile = file_path)
}

df <- read.csv(file_path, sep = ";")
```
\

#### Data Summary

```{r}
summary(df)
```

#### Visualising Correlations

There is a way to plot all multivariate correlations (although visually not so appealing). It also does not work, if there are many dimensions. You need to subset the results.

```{r}
pairs(df[, 8:12])
```

We create our own visualisation, that is much better to read.

```{r}
df_scaled <- df %>% 
  scale() %>% 
  as.tibble()

df_gather <- df_scaled %>% 
  gather(key = "variable", value = "value", 1:11) %>% 
  mutate(variable = as.factor(variable))

g <-ggplot(df_gather, aes(x = quality, y = value))
g <- g + facet_wrap( ~ variable)
g <- g + geom_point()
g <- g + geom_smooth(se = F, method = "lm")
g
```

#### Correlation Matrix

Assuming there is a linear relationship between variables, a correlation matrix is calculated.

```{r}
cor_vals <- cor(df) %>% 
  as.data.frame() %>% 
  dplyr::mutate(Var1 = rownames(.)) %>% 
  gather(key = "Var2", value = "Corr", 1:12)
g <- ggplot(cor_vals, aes(x = Var1, y = Var2, fill = Corr))
g <- g + geom_tile()
g <- g + scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation")
g <- g + theme(axis.text.x = element_text(angle = 90, hjust = 1))
g
```

Also here we can make use of a simpler alternative: **corrplot()**.

```{r}
cor_df <- cor(df)
corrplot.mixed(cor_df)
```

This shows in the upper right triangle correlations as circles. Size corresponds to absolute value of correlation. We are looking for large circles, because they indicate high absolute correlations. The colors correspond to positive or negative correlations. Positive correlation between X and Y means, that an increase of X leads to an increase in Y. A negative correlation between X and Y means, that an increase of X leads to a decrese in Y.

### Modeling

> Model Setup

We create a model with **lm()**.

```{r model_setup}
model <- lm(formula = "quality ~ .", data = df)
```

```{r}
summary(model)
```

We see which parameters are statistically relevant, and what the parameter values are.

> Predictions

Now, we can create predictions.

```{r}
df$quality_pred <- predict(object = model, 
                              newdata = df)
```

Predicted values and true values are visualised as correlation plot. A linear regression line is drawed for reference. Also a black line is drawed as reference for a perfect regression, in which predicted values and actual values are identical. 

```{r}
g <- ggplot(df, aes(y = quality, x = quality_pred))
g <- g + geom_point(alpha = .1)
g <- g + geom_smooth(method = "lm", se = F)
g <- g + geom_abline(slope = 1, intercept = 0)
g <- g + ylab ("Actual")
g <- g + xlab ("Prediction")
g <- g + ggtitle ("Prediction vs. Actual")
g
```

Predicted regression line nearly matches vertical line. This means there is hardly any bias. But the variation is quite high.

> Model Performance

We calculate adjusted R-squared to analyse model performance. R-squared is a measure that indicates how much of variability in data is explained by the model.

```{r}
model_summary <- summary(model)
model_summary$adj.r.squared
```

Only 35 % of variability in the data is explained by the model. That is rather poor, so we should think about some more complex model.

You should use adjusted R-squared, because R-squared always increases when more explanatory variables are added to a model. Its value will always be less or equal to R-squared.

Model quality is far from perfect, but reasonably good.

> Error Independence

The residuals of the model should be normally distributed. We can check this based on a QQ-plot. We extract the residuals and visualise it with **qqnorm()**. **qqline()** adds a reference line. We assume linearity if all points are on this line.

```{r}
res <- residuals(object = model)
qqPlot(res)
```






\
\

# Exercise 1
\

Pick one of the following numbers to active the link to a specific publication, using MLR. Try to describe that publication in a short paragraph, define why MLR was useful in that particular case and name an additional (chemistry/biology-related) problem that could be solved in a similar manner (with MLR).
\
\
(!!!!!!!!!!!!!!!!!!!!)
Additionally, find a publication on a similar topic (like the same type of disease data, or the same enzyme regarded etc.) using MULTIVARIATE regression and explain how different can the approaches be and how differently they allow us to look at that scientific problem.

\
\
Publications to choose from for MLR:
\

[1](https://www.sciencedirect.com/science/article/pii/S2211467X19301208), [2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8562410/), [3](https://www.sciencedirect.com/science/article/pii/S0022286021007043), [4](https://archivepp.com/storage/models/article/rd5ep1H7eQzPsCKRZ5NNwUvhbFiS0oaByUey6D0qS4jVz5hJs69yCzgIxk39/qsar-study-of-novel-indole-derivatives-in-hepatitis-treatment-by-stepwise-multiple-linear-regressi.pdf), [5](https://www.mdpi.com/1420-3049/25/13/3088), [6](https://revues.imist.ma/index.php/morjchem/article/view/33059), [7](https://link.springer.com/article/10.1007/s11030-021-10181-y), [8](https://www.cell.com/heliyon/pdf/S2405-8440(20)31358-X.pdf), [9](http://ijopaar.com/files/IssueContent/D-A18117-19021.pdf), [10](https://www.tandfonline.com/doi/abs/10.1080/07391102.2022.2109753), [11](https://pubs.rsc.org/en/content/articlehtml/2021/nj/d0nj05298a),[12](https://www.sciencedirect.com/science/article/pii/S1878535222005202), [13](https://doi.org/10.1145/3522664.3528606), [14](https://www.tandfonline.com/doi/abs/10.1080/1062936X.2023.2171478), [15](https://www.sciencedirect.com/science/article/abs/pii/S2214785320358612), [16](https://www.sciencedirect.com/science/article/abs/pii/S0147651319311534), [17](https://link.springer.com/article/10.1007/s11356-021-12494-9), [18](https://www.sciencedirect.com/science/article/pii/S0169743922001988)



# Examples of using MLR, with code and description
\

Good examples of using MLR with description:
\

[1](https://github.com/lokaas/MultipleLinearRegression/blob/main/multiple_linear_regression.ipynb)

[2](https://github.com/cyranothebard/multiple_linear_regression/blob/master/M01S11.ipynb)

[3](https://carpentries-incubator.github.io/multiple-linear-regression-public-health/04-assumptionsAndFit/index.html)

\

Use one of those description in order to create your own script for MLR, either Python or R, whatever you prefer (choose only one!!). If it's still problematic for you, go further to GitHub examples and finish your script in Exercise 3, after you're done with Exercise 2.
\
\

# Exercise 2
\
\

Now, go through 1 GitHub MLR practical example and describe it in a paragraph. 
\
\



## GitHub examples:
\

\

https://github.com/Avinav09/Multiple-Linear-Regression/tree/master

https://github.com/cskaust/MultipleLinearRegressionInR/blob/main/%23%20Multiple%20Linear%20Regression.r

https://github.com/pranavseth/Multiple-Linear-Regression-in-R

https://github.com/rinaldoclemente/Life-Expectancy-Dataset-Analysis/blob/master/Life%20Expectancy%20Report.pdf

https://github.com/jordancheah/MultipleLinearRegression-HackerRankPredictHousePrices

https://github.com/shivangidx/Multiple-Linear-Regression-Analysis

https://github.com/carpentries-incubator/multiple-linear-regression-public-health

https://github.com/shivanshjoshi28/Machine-Learning-project-1

https://github.com/jasonx1011/temperature-prediction

https://github.com/acmyers/chillerMLR

https://github.com/womenindatascienceatx/multiple-linear-regression-and-gradient-descent/blob/master/ch15ch08-Final.ipynb

https://github.com/mnassrib/multiple-linear-regression

https://github.com/DeltaOptimist/Linear_Regression_Simple_Multiple_Using_R

https://github.com/allisonhorst/esm-206-lab-9/blob/master/lab_9_template.Rmd

https://github.com/mahesh147/Multiple-Linear-Regression/blob/master/multiple_linear_regression.py

https://github.com/Ashleshk/Machine-Learning-Stanford-Andrew-Ng

https://github.com/mrqasimasif/Multiple-Linear-Regression-CO2-Emission-Prediction

https://github.com/datasciencewithsan/Multiple-Linear-Regression

https://github.com/NohaWaly/MultipleLinearRegression/blob/master/MultipleLinearRegression.py

https://github.com/Bidhisha24/Multiple-Linear-Regression-Model-Checking-and-Diagnostics







# Exercise 3
\

Create your own script for MLR based on the examples and further use it on a data set of your choosing (it can be a recycled data set - the one you already used before during classes - ours or others).

\
\


Send finished exercises (Word file+code file or Jupyter notebook or Rmarkdown) to my email: __klaudia.chmielewska@ug.edu.pl__ with the Title format: "DCH2_Name_Class3" (DCH = Digital Chemistry)

***


\
\
Klaudia Chmielewska

_klaudia.chmielewska@ug.edu.pl_


\
\

See you in the next lesson!